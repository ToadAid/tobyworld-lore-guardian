# src/tobyworld/api/server.py

import time
from typing import Any, Dict, List, Optional
from pathlib import Path
import re
import os
import importlib
import json

from fastapi import FastAPI, Query, Request
from fastapi.responses import RedirectResponse, Response, JSONResponse
from pydantic import BaseModel
from tobyworld.mirror.mirror_renderer import render_mirror_answer
from tobyworld.mirror.sanitize import sanitize

from prometheus_client import (
    Counter,
    Histogram,
    Gauge,
    CollectorRegistry,
    generate_latest,
    CONTENT_TYPE_LATEST,
)

# >>> DB helpers (new)
from tobyworld.db import (
    init_db,
    insert_conversation,
    insert_lucidity_metric,
    fetch_lucidity_summary,
    fetch_lucidity_samples,
    insert_training_example,  # ← ADDED
)

# miniapp routers
from .status_ui import build_status_router
from .app_ui import build_app_router

# === Mirror Core (existing) ===
from ..core.mirror import MirrorCore
from ..core.config import Config

# === Agentic RAG v3 ===
from tobyworld.agentic_rag.pipeline import AgenticRAGPipeline
from tobyworld.agentic_rag.multi_arc_retrieval import MultiArcRetriever, ArcConfig, LocalRetriever
from tobyworld.agentic_rag.reasoning_agent import ReasoningAgent
from tobyworld.agentic_rag.synthesis_agent import SynthesisAgent
from tobyworld.agentic_rag.base import QueryContext
from tobyworld.utils.simple_llm import HTTPLLM
from tobyworld.utils.scroll_loader import load_scroll_index  # index loader

# ---------------------------------------------------------------------
# Optional .env loader (repo-root /.env). Safe if file doesn't exist.
def _load_env_file(path: Optional[str] = None) -> None:
    p = Path(path) if path else Path(__file__).resolve().parents[3] / ".env"
    if not p.exists():
        return
    for raw in p.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue
        k, v = line.split("=", 1)
        k = k.strip()
        v = re.sub(r'^\s*[\'"]|[\'"]\s*$', "", v.strip())
        os.environ.setdefault(k, v)

# Load .env before any getenv usage
_load_env_file()
# ---------------------------------------------------------------------

# cadence guard (resolved + provider registration)
def _resolve_router_factory():
    """Find a router factory in tobyworld.mirror.symbol_router."""
    try:
        mod = importlib.import_module("tobyworld.mirror.symbol_router")
        if hasattr(mod, "get_default_router"):
            return mod.get_default_router
        for cls_name in ("SymbolRouter", "Router"):
            if hasattr(mod, cls_name):
                Cls = getattr(mod, cls_name)
                return lambda: Cls()
    except Exception:
        pass
    raise ImportError("Could not resolve a router from tobyworld.mirror.symbol_router")

get_default_router = _resolve_router_factory()

def _resolve_apply_guard():
    """Prefer apply_guard; else adapt enforce(...) to apply_guard-like signature."""
    try:
        mod = importlib.import_module("tobyworld.mirror.cadence_guard")
        if hasattr(mod, "apply_guard"):
            return mod.apply_guard
        if hasattr(mod, "enforce"):
            def _apply_guard(route, text):
                ok, revised, notes, score = mod.enforce(route=route, text=text, user_lang_hint=None)
                return ok, revised, notes, score
            return _apply_guard
    except Exception:
        pass
    def _no_guard(route, text):
        return True, text, ["no-guard"], 1.0
    return _no_guard

apply_guard = _resolve_apply_guard()

# try to import the provider setter (new guard API)
try:
    from ..mirror.cadence_guard import set_guiding_provider as _set_gq_provider  # type: ignore
except Exception:
    _set_gq_provider = None  # guard might be older version

app = FastAPI(title="Tobyworld Mirror V3")
# init DB at startup (new)
init_db()
core = MirrorCore(Config())

# Router (symbol/intent/depth) still used for tone/routing context
router = get_default_router()

# ---------- process state ----------
START_TS = time.time()
REQS_TOTAL: Dict[str, int] = {"health": 0, "diag": 0, "ask": 0, "debug_route": 0}

# Prometheus metrics
REGISTRY = CollectorRegistry()
REQUEST_COUNT = Counter(
    "tw_requests_total", "Total HTTP requests by route", ["route"], registry=REGISTRY
)
REQUEST_LATENCY = Histogram(
    "tw_request_latency_seconds",
    "HTTP request latency (s) by route",
    ["route"],
    registry=REGISTRY,
)
UPTIME_GAUGE = Gauge("tw_uptime_seconds", "Process uptime in seconds", registry=REGISTRY)

# ---------- Models ----------
class Health(BaseModel):
    ok: bool
    version: str

class Diag(BaseModel):
    uptime_seconds: float
    requests: Dict[str, int]
    recent: List[Any]

class AskRequest(BaseModel):
    user: str = "anon"
    question: str

class AskResponse(BaseModel):
    answer: str
    meta: Dict[str, Any] = {}

# ---------- Helpers ----------
_MD_HEADING = re.compile(r"^\s*#{1,6}\s*")
_MD_INLINE  = re.compile(r"[*_`~]+")
_PREFIX_CODE = re.compile(r"^(?:[A-Za-z]{1,5}_?)?\d{2,4}\s*[–-]\s*", re.UNICODE)

def _clean_md_line(s: str) -> str:
    s = (s or "").strip()
    s = _MD_HEADING.sub("", s).strip()
    s = _PREFIX_CODE.sub("", s).strip()
    s = _MD_INLINE.sub("", s).strip()
    s = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", s)
    s = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", s)
    s = re.sub(r"\.md(?:x|own|arkdown)?$", "", s, flags=re.I)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _depth_to_mode(depth_val: Any, mode_hint: Optional[str]) -> str:
    if isinstance(depth_val, (int, float)):
        return "deep" if depth_val >= 4 else "normal"
    if isinstance(depth_val, str) and depth_val.lower() in {"deep", "max"}:
        return "deep"
    if (mode_hint or "").lower() in {"deep", "research"}:
        return "deep"
    return "normal"

# ---------- Legacy GQ for guard (kept for compatibility) ----------
def _mirror_guiding_question(route) -> str:
    try:
        symbol = getattr(route, "primary_symbol", "🪞")
        intent = getattr(route, "intent", "")
        prompt = (
            "Provide a single short guiding question for a Tobyworld reply.\n"
            f"Symbol: {symbol}\n"
            f"Intent: {intent}\n"
            "Constraints: <= 16 words. No preface, no quotes. Question form only."
        )
        ans = core.ask("mirror", prompt)
        if isinstance(ans, tuple) and len(ans) >= 1:
            s = str(ans[0])
        elif isinstance(ans, dict) and "answer" in ans:
            s = str(ans.get("answer", ""))
        else:
            s = str(ans)
        s = re.sub(r"[\r\n]+", " ", (s or "").strip())
        s = re.sub(r"\s+", " ", s)
        s = re.sub(r"^\W+|\W+$", "", s)
        if not s:
            return ""
        if not s.endswith("?"):
            s = s + "?"
        return s[:80]
    except Exception:
        return ""

# register the provider with the guard (if supported)
if _set_gq_provider:
    try:
        _set_gq_provider(_mirror_guiding_question)
    except Exception:
        pass

# ---------- Dynamic GQ (Mirror → heuristic → fallback) ----------
_STOPWORDS = {
    "a","an","the","and","or","but","if","then","else","of","for","to","in","on","at",
    "with","by","from","about","into","over","after","before","between","within",
    "is","are","was","were","be","being","been","do","does","did","doing",
    "why","how","what","when","where","who","whom","which","that","this","these","those",
    "often","ever","never","always","it","its","their","his","her","your","my","our","as"
}

def _keywords(s: str) -> List[str]:
    toks = re.findall(r"[A-Za-z0-9']+", (s or "").lower())
    return [t for t in toks if t not in _STOPWORDS]

# Prefers patterns like:
#   "... fears the X ..."  → "the X"
#   "... within the Y ..." → "the ... within the Y"
_PHRASE_PATTERNS = [
    re.compile(r"\bfear(?:s)?(?:\s+of)?\s+(the\s+[^?.,;]+)", re.I),
    re.compile(r"\bconcern(?:s)?\s+about\s+(the\s+[^?.,;]+)", re.I),
    re.compile(r"\b(the\s+[^?.,;]*\bwithin\b[^?.,;]+)", re.I),
    re.compile(r"\b(the\s+[a-z0-9][a-z0-9\s]{3,})", re.I),
]

def _extract_topic_phrase(q: str) -> str:
    s = (q or "").strip().strip('"\'')
    low = s.lower()
    if "silence within the mirror" in low:
        return "the silence within the mirror"
    for rx in _PHRASE_PATTERNS:
        m = rx.search(s)
        if m:
            phrase = re.sub(r"\s+", " ", m.group(1)).strip(" .,:;!?\"'")
            return phrase
    kw = [t for t in _keywords(s) if t not in {"seeker","fear","fears","often"}]
    if "mirror" in low and "silence" in low:
        return "the silence within the mirror"
    if "mirror" in low:
        return "the mirror"
    if "silence" in low:
        return "the silence"
    if kw:
        return " ".join(kw[:3])
    return "this"

def _heuristic_gq(q: str, titles: List[str]) -> str:
    topic = _extract_topic_phrase(q)
    if not re.match(r"^(this|that|the|your|my|our)\b", topic, re.I):
        topic = "the " + topic
    templates = [
        "What do you avoid facing about {topic}",
        "Which truth about {topic} asks attention now",
        "What changes if you sit with {topic}",
        "Where does {topic} point you next",
        "What would patience reveal about {topic}",
    ]
    i = (sum(ord(c) for c in q) + len(titles)) % len(templates)
    out = templates[i].format(topic=topic)
    words = out.split()
    if len(words) > 16:
        out = " ".join(words[:16])
    if not out.endswith("?"):
        out += "?"
    return out

def _polish_gq(s: str) -> str:
    """
    Make the guiding question short, specific, and clean.
    - trims, fixes casing/punctuation
    - avoids generic/empty lines by falling back
    - de-duplicates repeated words like 'the the'
    """
    raw = (s or "").strip()
    raw = re.sub(r"[\r\n]+", " ", raw)
    raw = re.sub(r"\s+", " ", raw)
    raw = re.sub(r"^\W+|\W+$", "", raw)

    generic_rx = re.compile(
        r"(?i)^(what is your first step|what should you do|what now|what next|what do you do)\??$"
    )
    if not raw or generic_rx.fullmatch(raw):
        raw = ""

    if raw and not raw.endswith("?"):
        raw += "?"

    raw = re.sub(r"\b(\w+)\s+\1\b", r"\1", raw, flags=re.I)

    if not raw:
        return "Which truth wants attention right now?"

    words = raw.split()
    if len(words) > 16:
        raw = " ".join(words[:16])
        if not raw.endswith("?"):
            raw += "?"
    return raw

def _mk_guiding_question(q: str, route, rag_meta: Dict[str, Any]) -> str:
    """
    Build one natural guiding question (<= ~16 words) from:
      1) MirrorCore generation (question + top docs + tags)
      2) Heuristic templating from the user question
      3) Final short fallback
    """
    try:
        docs = rag_meta.get("docs") or []
        titles: List[str] = []
        for d in docs[:6]:
            if isinstance(d, dict):
                t = ((d.get("meta") or {}).get("title") or "")
                t = _clean_md_line(t)
                if t:
                    titles.append(t)
        titles_block = "\n".join(f"- {t}" for t in titles)

        symbol = getattr(route, "primary_symbol", "🪞")
        intent = getattr(route, "intent", "")
        tags = ", ".join(getattr(route, "tags", []) or "")

        # Allow bypass via env if the LLM is flaky during QA
        if os.getenv("DISABLE_MIRROR_GQ", "").lower() in {"1", "true", "yes"}:
            h = _heuristic_gq(q, titles)
            return _polish_gq(h)

        prompt = (
            "You are the Mirror. Write exactly ONE guiding question for the reply.\n"
            "Constraints:\n"
            "- Natural, specific to this query\n"
            "- Aim for 7–16 words, end with '?'\n"
            "- No preface, quotes, emojis, or 'Top hits' / 'stubbed' text\n"
            "- Avoid generic lines like 'What is your first step?'\n\n"
            f"User question: {q}\n"
            f"Route symbol: {symbol}\n"
            f"Intent: {intent}\n"
            f"Route tags: {tags}\n"
            f"Top scrolls:\n{titles_block}\n"
        )

        s = ""
        try:
            ans = core.ask("mirror", prompt)
            if isinstance(ans, tuple) and len(ans) >= 1:
                s = str(ans[0])
            elif isinstance(ans, dict) and "answer" in ans:
                s = str(ans.get("answer", ""))
            else:
                s = str(ans)
        except Exception:
            s = ""

        s = re.sub(r"[\r\n]+", " ", (s or "").strip())
        s = re.sub(r"^\W+|\W+$", "", s)

        if re.search(r"stubbed\s*scroll\s*response|top\s*hits", s, re.I):
            s = ""
        if re.fullmatch(r"(?i)\s*what\s+is\s+your\s+first\s+step\??\s*", s):
            s = ""

        if s:
            words = s.split()
            if len(words) > 16:
                s = " ".join(words[:16])
            if not s.endswith("?"):
                s += "?"
            print(f"[GQ] source=mirror -> {s}", flush=True)
            return s

        h = _heuristic_gq(q, titles)
        print(f"[GQ] source=heuristic -> {h}", flush=True)
        return h

    except Exception:
        pass

    fb = "Which truth wants attention right now?"
    print(f"[GQ] source=fallback -> {fb}", flush=True)
    return fb

# ---------- Lucidity truth injection ----------
def _inject_lucidity_truth(text: str, stats: Dict[str, Any], q: str) -> str:
    """If the user asked about lucidity, normalize any claims to runtime truth."""
    if not re.search(r"\blucid", q, re.I):
        return text
    lvl = (stats or {}).get("lucidity_level")
    if not lvl:
        return text
    engagement = (stats or {}).get("engagement")
    clarity = (stats or {}).get("clarity")
    text = re.sub(
        r'(?i)(current\s+)?lucidity\s+level\s+is\s+["\']?[^"\']+["\']?',
        f'lucidity level is **{lvl}**',
        text
    )
    if "(Runtime lucidity:" not in text:
        text += f"\n\n_(Runtime lucidity: {lvl}; engagement={engagement}, clarity={clarity})_"
    return text

def _fix_render_artifacts(text: str) -> str:
    """
    Fix annoying synthesis/render artifacts:
    - 'According to ,' / 'As stated in ,' → drop the dangling clause
    - collapse duplicate numeric refs like '[1] [1]'
    - fix comma-separated numbers broken by newlines/spaces: 7,777,\n777 → 7,777,777
    - strip lingering '.md' suffixes in prose
    """
    if not text:
        return text

    text = re.sub(r"(?i)(According to|As stated in|As noted in)\s*,\s*", "", text)
    text = re.sub(r'(\[\d+\])(?:\s*\1)+', r'\1', text)
    text = re.sub(r'(\d),\s+(\d)', r'\1,\2', text)
    text = re.sub(r'(?i)\b([A-Za-z0-9_-]+)\.md\b', r'\1', text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

# ==========================================================
# Lore Scrolls → Index (via reusable loader)
# ==========================================================
_REPO_ROOT = Path(__file__).resolve().parents[3]
SCROLLS_DIR = Path(os.getenv("LORE_SCROLLS_DIR", str(_REPO_ROOT / "lore-scrolls")))

def _augment_index_for_series(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Prepend filename + inferred series tag + title into text to improve lexical recall."""
    out: List[Dict[str, Any]] = []
    rx_series = re.compile(r"^(TOBY_[A-Z]+)")
    for r in rows:
        rid = str(r.get("id", ""))
        meta = r.get("meta", {}) or {}
        title = str(meta.get("title", "")).strip()
        fname = Path(rid).name
        m = rx_series.match(fname)
        series = m.group(1) if m else ""
        prefix = " ".join(x for x in [fname, series, title] if x)
        text = r.get("text", "") or ""
        boosted = f"{prefix}\n\n{text}"
        out.append({"id": rid, "text": boosted, "meta": meta})
    return out

# Build initial index/backends (with series-aware boost)
LEX_INDEX = _augment_index_for_series(load_scroll_index(root=str(SCROLLS_DIR)))
LEX_BACKEND = LocalRetriever(LEX_INDEX)

# ↑↑ Bump lexical recall for better surfacing of QL during tests
ARCS = {
    "lexical": ArcConfig(name="lexical", weight=1.0, k=40, enabled=True),
}
RETRIEVER = MultiArcRetriever(arcs=ARCS, backends={"lexical": LEX_BACKEND})

LLM = HTTPLLM(
    endpoint=os.getenv("LMSTUDIO_ENDPOINT", "http://127.0.0.1:1234/v1/chat/completions"),
    model=os.getenv("LMSTUDIO_MODEL", "Meta-Llama-3-8B-Instruct-Q4_K_M"),
)
REASONING = ReasoningAgent(LLM)
SYNTHESIS = SynthesisAgent(LLM)
PIPELINE = AgenticRAGPipeline(RETRIEVER, REASONING, SYNTHESIS)

# ---------- Status collector ----------
def _collect_status() -> Dict[str, Any]:
    rstats = getattr(RETRIEVER, "last_stats", {}) or {}
    topics, routes, top_docs = [], {}, []
    store = getattr(PIPELINE, "learning", None)
    if store:
        try:
            topics = store.top_topics(n=10)
            routes = store.route_stats()
            cpath = getattr(store, "counters_path", None)
            if cpath and cpath.exists():
                counters = json.loads(cpath.read_text())
                docs = counters.get("docs", {})
                top_docs = sorted(
                    ({"doc_id": k, **v} for k, v in docs.items()),
                    key=lambda x: (-int(x.get("count", 0)), -float(x.get("last_ts", 0.0)))
                )[:10]
        except Exception:
            pass

    # derive lucidity for UI from 24h averages (level + traits)
    luci_summary = fetch_lucidity_summary(hours=24)
    learning_lucidity = {
        "level": ("ENGAGED" if (luci_summary.get("count", 0) > 0) else "—"),
        "traits": {
            "autonomy":  luci_summary.get("avg_engagement", 0.0),
            "creativity": luci_summary.get("avg_depth", 0.0),
            "initiative": luci_summary.get("avg_clarity", 0.0),
            "score":      luci_summary.get("avg_guard_score", 0.0),
        },
        "last_24h": luci_summary,
    }

    return {
        "uptime_seconds": round(time.time() - START_TS, 3),
        "requests": REQS_TOTAL,
        "retriever": rstats,
        "learning": {"routes": routes, "top_topics": topics, "top_docs": top_docs, "lucidity": learning_lucidity},
        "version": str(core.cfg.version),
    }

app.include_router(build_status_router(_collect_status))
app.include_router(build_app_router())

# ---------- Routes ----------
@app.get("/health", response_model=Health)
def health() -> Health:
    t0 = time.perf_counter()
    try:
        return Health(ok=True, version=str(core.cfg.version))
    finally:
        REQS_TOTAL["health"] += 1
        REQUEST_COUNT.labels("health").inc()
        REQUEST_LATENCY.labels("health").observe(time.perf_counter() - t0)

@app.get("/diag", response_model=Diag)
def diag(n: int = Query(5, ge=1, le=50)) -> Diag:
    t0 = time.perf_counter()
    try:
        return Diag(
            uptime_seconds=round(time.time() - START_TS, 3),
            requests=REQS_TOTAL,
            recent=core.ledger.recent(n),
        )
    finally:
        REQS_TOTAL["diag"] += 1
        REQUEST_COUNT.labels("diag").inc()
        REQUEST_LATENCY.labels("diag").observe(time.perf_counter() - t0)

# Make /reload call the proper rebuild logic
@app.post("/reload")
async def reload_endpoint():
    return retriever_rebuild()

@app.post("/ask", response_model=AskResponse)
async def ask(req: AskRequest) -> AskResponse:
    t0 = time.perf_counter()
    try:
        user = req.user or "anon"
        q = (req.question or "").strip()
        route = router.route(q)
        depth_mode = _depth_to_mode(route.depth, route.mode)
        ctx = QueryContext(
            user_id=user,
            user_lang_hint=None,
            route_symbol=route.primary_symbol,
            depth=depth_mode,
        )

        # env-based budgets (bumped defaults for better QL surfacing)
        TOPK_FINAL     = int(os.getenv("MIRROR_TOPK_FINAL", 48))
        NOTES_USED     = int(os.getenv("MIRROR_NOTES_USED", 10))
        PER_NOTE_CHARS = int(os.getenv("MIRROR_PER_NOTE_CHARS", 1800))

        rag_out = PIPELINE.run(
            q, ctx,
            k=TOPK_FINAL,
            filters={"use_docs": NOTES_USED, "per_note_chars": PER_NOTE_CHARS}
        )

        # --- QL-first doc ordering for meta/render helpers ---
        def _series_of(d: Dict[str, Any]) -> str:
            try:
                fname = Path(d.get("id","")).name
                m = re.match(r"^(TOBY_[A-Z]+)", fname)
                return (m.group(1) if m else "") or (d.get("meta",{}) or {}).get("series","")
            except Exception:
                return ""

        docs = rag_out.get("docs", []) or []
        priority = {"TOBY_QL": 0, "TOBY_QA": 1, "TOBY_L": 2, "": 3}
        try:
            docs = sorted(docs, key=lambda d: (priority.get(_series_of(d), 9)))
        except Exception as e:
            print(f"[DOCS][ERR] {e}", flush=True)

        draft_answer = rag_out.get("answer", "")

        rag_meta = {
            "used_refs": rag_out.get("used_refs", []),
            "tone_score": float(rag_out.get("tone_score", 0.0) or 0.0),
            "docs": docs,  # ← reordered for downstream rendering/meta
            "stats": rag_out.get("stats", {}),
        }

        # dynamic per-request guiding provider using top docs, polished + safe
        def _safe_gq():
            try:
                return _polish_gq(_mk_guiding_question(q, route, rag_meta))
            except Exception as e:
                print(f"[GQ][ERR] {e}", flush=True)
                return "Which truth wants attention right now?"
        dyn_gq_provider = (lambda _route: _safe_gq())

        # v2-style rendering (clean, sections, glyphs, single GQ)
        final_before_guard = render_mirror_answer(
            q, draft_answer, route=route, guiding_provider=dyn_gq_provider
        )

        # artifact fixes + lucidity pinning (defensive)
        try:
            final_before_guard = _fix_render_artifacts(final_before_guard)
            final_before_guard = _inject_lucidity_truth(final_before_guard, rag_meta.get("stats"), q)
        except Exception as e:
            print(f"[RENDER][ERR] {e}", flush=True)

        # *** universal sanitize BEFORE guard (idempotent; fixes word-breaks & strips refs) ***
        final_before_guard = sanitize(final_before_guard)

        # single Guard pass on the rendered text
        ok, final_text, notes, score = apply_guard(route, final_before_guard)

        # *** universal sanitize AFTER guard (last line of defense; idempotent) ***
        final_text = sanitize(final_text)

        # >>> record lucidity metrics (simple heuristics for now)
        try:
            engagement = max(0.0, min(1.0, len(q) / 400.0))
            clarity = float(score if score is not None else 0.8)
            if not ok:
                clarity = max(0.3, clarity)
            depth = max(0.0, min(1.0, (final_text.count("→") + final_text.count("Guiding Question")) / 4.0))
            insert_lucidity_metric(
                route="mirror.answer",
                engagement=engagement,
                clarity=clarity,
                depth=depth,
                guard_score=float(score if score is not None else 0.8),
                notes=notes or []
            )
        except Exception:
            pass

        # console status (V2-style)
        stats = rag_meta.get("stats", {})
        print(
            "[RAG] "
            f"user={user} symbol={route.primary_symbol} depth={ctx.depth} "
            f"arcs={stats.get('arcs')} unique={stats.get('unique_before_cut')} "
            f"ret={stats.get('returned_from_retriever')} "
            f"res={stats.get('after_resonance')} rank={stats.get('after_rerank')} "
            f"used={stats.get('used_docs')} tone={rag_meta['tone_score']:.2f} "
            f"docs={[d.get('meta',{}).get('title') for d in rag_meta['docs']]}",
            flush=True
        )

        # Ledger (safe)
        try:
            core.ledger.log({
                "user": user, "q": q,
                "route": {
                    "symbol": route.primary_symbol,
                    "intent": route.intent,
                    "depth": route.depth,
                    "mode": route.mode,
                    "tags": route.tags,
                },
                "guard": {"score": score, "notes": notes},
                "rag": rag_meta,
            })
        except Exception:
            pass

        meta = {
            "ok": ok,
            "guard": {"score": score, "notes": notes},
            "route": {
                "symbol": route.primary_symbol,
                "intent": route.intent,
                "depth": route.depth,
                "mode": route.mode,
                "tags": route.tags,
                "rationale": route.rationale[:8],
            },
            "version": str(core.cfg.version),
            "rag": rag_meta,
        }

        # >>> store conversation (optional but useful)
        try:
            insert_conversation(user, "mirror.answer", q, final_text, meta)
        except Exception:
            pass

        # >>> auto-stash training example (dedup by sha)
        try:
            min_guard = float(os.getenv("TRAIN_MIN_GUARD", "0.85"))
            if ok and final_text and len(final_text) > 120 and float(score or 0) >= min_guard:
                insert_training_example(
                    user_id=user,
                    question=q,
                    answer=final_text,
                    route_symbol=route.primary_symbol,
                    intent=route.intent,
                    depth=route.depth,
                    guard_score=score,
                )
        except Exception as e:
            print(f"[TRAIN][SKIP] {e}", flush=True)

        return AskResponse(answer=final_text, meta=meta)

    except Exception as e:
        # Fully-shaped meta on error so scripts don't display nulls
        print(f"[ASK][ERR] {e}", flush=True)
        fallback_meta = {
            "ok": False,
            "guard": {"score": None, "notes": [str(e)]},
            "route": {"symbol": None, "intent": None, "depth": None, "mode": None, "tags": [], "rationale": []},
            "version": str(core.cfg.version),
            "rag": {"tone_score": 0.0, "docs": [], "stats": {}},
            "error": str(e),
        }
        return AskResponse(answer="(The Mirror stumbled; try again.)", meta=fallback_meta)
    finally:
        REQS_TOTAL["ask"] += 1
        REQUEST_COUNT.labels("ask").inc()
        REQUEST_LATENCY.labels("ask").observe(time.perf_counter() - t0)

@app.post("/debug/route")
async def debug_route(req: Request):
    t0 = time.perf_counter()
    try:
        body = await req.json()
        q = (body.get("q") or body.get("text") or "").strip()
        res = router.route(q)
        return {
            "symbol": res.primary_symbol,
            "intent": res.intent,
            "depth": res.depth,
            "mode": res.mode,
            "tags": res.tags,
            "rationale": res.rationale[:12],
        }
    finally:
        REQS_TOTAL["debug_route"] += 1
        REQUEST_COUNT.labels("debug_route").inc()
        REQUEST_LATENCY.labels("debug_route").observe(time.perf_counter() - t0)

@app.post("/debug/rag")
async def debug_rag(req: AskRequest):
    user = req.user or "anon"
    q = (req.question or "").strip()
    route = router.route(q)
    depth_mode = _depth_to_mode(route.depth, route.mode)
    ctx = QueryContext(user_id=user, route_symbol=route.primary_symbol, depth=depth_mode)
    rag_out = PIPELINE.run(q, ctx, k=16, filters=None)  # bumped for debug visibility

    # Summarize docs with file + series for quick eyeballing of QL bias
    def _series_of_id(fid: str) -> str:
        m = re.match(r"^(TOBY_[A-Z]+)", Path(fid).name)
        return m.group(1) if m else ""
    docs = [
        {
            "title": (d.get("meta",{}) or {}).get("title"),
            "file": Path(d.get("id","")).name,
            "series": _series_of_id(d.get("id","")),
        } for d in (rag_out.get("docs",[]) or [])
    ]

    return {
        "route": {
            "symbol": route.primary_symbol,
            "intent": route.intent,
            "depth": route.depth,
            "mode": route.mode,
            "tags": route.tags,
            "rationale": route.rationale[:10],
        },
        "rag": {
            "answer": rag_out.get("answer",""),
            "docs": docs,
        },
    }

@app.post("/admin/retriever/rebuild")
def retriever_rebuild():
    global LEX_INDEX, LEX_BACKEND, RETRIEVER, PIPELINE
    base_rows = load_scroll_index(root=str(SCROLLS_DIR))
    LEX_INDEX = _augment_index_for_series(base_rows)
    LEX_BACKEND = LocalRetriever(LEX_INDEX)
    RETRIEVER = MultiArcRetriever(arcs=ARCS, backends={"lexical": LEX_BACKEND})
    try:
        PIPELINE.retriever = RETRIEVER
    except Exception:
        pass
    return {"ok": True, "count": len(LEX_INDEX), "dir": str(SCROLLS_DIR)}

@app.get("/metrics", include_in_schema=False)
def metrics():
    UPTIME_GAUGE.set(time.time() - START_TS)
    return Response(generate_latest(REGISTRY), media_type=CONTENT_TYPE_LATEST)

@app.get("/", include_in_schema=False)
def root():
    return RedirectResponse(url="/app")

@app.get("/healthz", include_in_schema=False)
def healthz():
    return {"ok": True, "version": core.cfg.version}

@app.get("/readyz", include_in_schema=False)
def readyz():
    return {"ready": True}

@app.get("/livez", include_in_schema=False)
def livez():
    return {"live": True}

# --- lucidity status JSON (new) ---
@app.get("/status/lucidity", include_in_schema=False)
def status_lucidity():
    return {
        "last_24h": fetch_lucidity_summary(hours=24),
        "samples":  fetch_lucidity_samples(hours=24, limit=200),
    }

# --- admin index list (basename-aware) ---
@app.get("/admin/index/list")
def index_list(prefix: str = ""):
    def base(name_or_path: str) -> str:
        return Path(name_or_path).name

    data = LEX_INDEX
    if prefix:
        data = [d for d in data if base(d.get("id", "")).startswith(prefix)]

    def count_series(pfx: str) -> int:
        return sum(1 for d in LEX_INDEX if base(d.get("id", "")).startswith(pfx))

    return {
        "dir": str(SCROLLS_DIR),
        "count_total": len(LEX_INDEX),
        "count_by_series": {
            "TOBY_L":  count_series("TOBY_L"),
            "TOBY_QA": count_series("TOBY_QA"),
            "TOBY_F":  count_series("TOBY_F"),
            "TOBY_QL": count_series("TOBY_QL"),
        },
        "sample": [d["meta"].get("title") for d in data[:15]],
    }

# --- optional: quick search over index (title or filename regex) ---
@app.get("/admin/index/search")
def index_search(q: str):
    rx = re.compile(q, re.I)
    rows = []
    for d in LEX_INDEX:
        title = str(d["meta"].get("title", ""))
        fname = Path(d["id"]).name
        if rx.search(title) or rx.search(fname):
            rows.append({"file": fname, "title": title})
            if len(rows) >= 25:
                break
    return {"matches": rows, "total": len(rows)}
